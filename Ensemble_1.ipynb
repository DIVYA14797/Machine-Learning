{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyj/wYwm030Y8TUtr/y3Wr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIVYA14797/Machine-Learning/blob/main/Ensemble_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_IasiPe10oc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is an ensemble technique in ML ?\n",
        "\n"
      ],
      "metadata": {
        "id": "UU8p39Oa11ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, an ensemble technique refers to a method that combines multiple models to produce better predictive performance than could be obtained from any of the constituent models alone. The underlying idea is to leverage the diversity among individual models to reduce errors and enhance overall accuracy and robustness.\n",
        "\n",
        "Ensemble techniques are popular because they often result in more accurate predictions and better generalization to unseen data compared to single models. There are several types of ensemble techniques, including:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating): This technique involves training multiple instances of the same base learning algorithm on different subsets of the training data. Each model is trained independently, and predictions are combined through averaging (for regression) or voting (for classification).\n",
        "\n",
        "2. Boosting: Boosting algorithms work sequentially, where each model tries to correct the errors made by the previous ones. Common boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), and XGBoost.\n",
        "\n",
        "3. Stacking: Stacking (short for stacked generalization) combines the predictions of multiple models as input features for a meta-model (also known as a blender or a meta-learner). The meta-model then learns how to best combine these predictions to make final predictions.\n",
        "\n",
        "4. Random Forest: Random Forest is a specific ensemble method based on decision trees. It creates a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "5. Voting: Voting methods combine predictions from multiple models by either taking the majority vote (for classification tasks) or averaging the predictions (for regression tasks).\n",
        "\n",
        "Ensemble techniques can significantly improve model performance, especially when individual models have different strengths and weaknesses or when there's a high variance in the data. However, they also come with increased complexity and computational costs compared to single models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NnR9q4Vo11fv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFTz0ed52TkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why are ensemble technique used in ML ?"
      ],
      "metadata": {
        "id": "NRb3VbZk11lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques are used in machine learning for several reasons, including:\n",
        "\n",
        "1. Improved Accuracy: Ensemble methods often result in higher predictive accuracy compared to individual models. By combining multiple models that may have different biases and variances, ensemble methods can mitigate errors and produce more robust predictions.\n",
        "\n",
        "2. Better Generalization: Ensemble techniques tend to generalize well to unseen data. By leveraging the diversity among individual models, ensembles can capture different aspects of the underlying data distribution, reducing overfitting and improving generalization performance.\n",
        "\n",
        "3. Robustness to Noise and Outliers: Ensemble methods are typically more robust to noisy data and outliers compared to single models. Since outliers may have less influence when combining predictions from multiple models, ensembles can produce more stable and reliable predictions in the presence of noisy or erroneous data.\n",
        "\n",
        "4. Handling Complex Relationships: In complex datasets with nonlinear relationships or high-dimensional feature spaces, individual models may struggle to capture all the underlying patterns. Ensemble methods, by combining multiple models with different perspectives, can better capture complex relationships and improve overall predictive performance.\n",
        "\n",
        "5. Model Interpretability: While some ensemble methods like Random Forests provide insights into feature importance, the interpretability of ensemble techniques can vary depending on the specific method used. However, in certain cases, ensembles can provide interpretability by aggregating predictions from multiple interpretable models.\n",
        "\n",
        "6. Reduction of Bias and Variance: Ensemble techniques can help balance bias and variance in model predictions. By combining models with different biases, ensemble methods can reduce overall bias, while combining models with different variances can reduce overall variance, resulting in more accurate and reliable predictions.\n",
        "\n",
        "Overall, ensemble techniques are valuable tools in the machine learning toolkit, offering a powerful approach to improving predictive performance, robustness, and generalization across various types of datasets and modeling tasks."
      ],
      "metadata": {
        "id": "Y76kz4IN11x-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7IRyx_F42z7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bagging ?"
      ],
      "metadata": {
        "id": "s-MsfFqD111d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning aimed at improving the stability and accuracy of models. It involves creating multiple subsets of the original dataset through bootstrapping (sampling with replacement) and training a separate model on each subset. The predictions from these models are then combined through averaging (for regression) or voting (for classification) to produce the final prediction.\n",
        "\n",
        "Here's how bagging works step by step:\n",
        "\n",
        "1. Bootstrapping: Random samples of the original dataset are created with replacement. This means that each subset may contain duplicate instances from the original dataset, and some instances may be omitted entirely. Typically, the size of each subset is the same as the size of the original dataset.\n",
        "\n",
        "2. Model Training: A base learning algorithm (e.g., decision trees, neural networks, or any other model) is trained independently on each bootstrapped subset. Since each subset is slightly different due to the random sampling process, each model learns slightly different patterns from the data.\n",
        "\n",
        "3. Prediction Aggregation: Once all models are trained, predictions are made on new data using each model. For regression tasks, the predictions from each model are averaged to obtain the final prediction. For classification tasks, the class labels predicted by each model are tallied, and the class with the most votes becomes the final prediction.\n",
        "\n",
        "The key idea behind bagging is that by training models on different subsets of the data and combining their predictions, the variability and overfitting of individual models are reduced, leading to better generalization performance. Bagging is particularly effective when the base learning algorithm tends to have high variance (i.e., it is prone to overfitting) or when the dataset is noisy.\n",
        "\n",
        "One of the most popular implementations of bagging is the Random Forest algorithm, which uses an ensemble of decision trees trained via bagging.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfr_-7nY27MY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XqryTaau3QRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is boosting ?"
      ],
      "metadata": {
        "id": "rc6CDiuK3Q9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is another ensemble technique in machine learning, like bagging, aimed at improving the predictive performance of models. Unlike bagging, which builds multiple models independently and then combines their predictions, boosting builds a sequence of models iteratively, with each model focusing on the instances that the previous models struggled with. Boosting aims to reduce bias and variance by sequentially fitting models to the residuals (or errors) of the preceding models.\n",
        "\n",
        "Here's how boosting works:\n",
        "\n",
        "1. Initialization: The process starts with an initial model, typically a simple one like a decision stump (a decision tree with only one split).\n",
        "\n",
        "2. Sequential Model Training: Subsequent models are trained iteratively. Each new model tries to correct the errors made by the combined predictions of the previous models. The algorithm pays more attention to instances that were misclassified or poorly predicted by the previous models.\n",
        "\n",
        "3. Weight Adjustment: Instances that are misclassified or have higher residuals are assigned higher weights during the training of subsequent models. This ensures that the subsequent models focus more on the instances that are harder to predict correctly.\n",
        "\n",
        "4. Combining Predictions: Once all models are trained, predictions are made by combining the predictions of all models, usually through a weighted sum or a weighted vote.\n",
        "\n",
        "Common boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), XGBoost, and LightGBM. These algorithms differ in how they update instance weights and how they combine the predictions of individual models.\n",
        "\n",
        "Boosting tends to achieve higher predictive accuracy compared to bagging, especially when applied to weak learners (models that perform slightly better than random guessing). However, boosting can be more prone to overfitting, especially if the base learners are too complex or if the dataset contains a large amount of noise. Regularization techniques and hyperparameter tuning are often used to mitigate overfitting in boosting algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Afdv-SBC3Te3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9EuM7bw3qaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the benefit of using ensemble techniques ?"
      ],
      "metadata": {
        "id": "2r8XEfft3q46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ensemble techniques in machine learning offers several benefits:\n",
        "\n",
        "1. Improved Accuracy: Ensemble techniques often result in higher predictive accuracy compared to individual models. By combining multiple models that may have different biases and variances, ensemble methods can mitigate errors and produce more robust predictions.\n",
        "\n",
        "2. Better Generalization: Ensemble methods tend to generalize well to unseen data. By leveraging the diversity among individual models, ensembles can capture different aspects of the underlying data distribution, reducing overfitting and improving generalization performance.\n",
        "\n",
        "3. Robustness to Noise and Outliers: Ensemble methods are typically more robust to noisy data and outliers compared to single models. Since outliers may have less influence when combining predictions from multiple models, ensembles can produce more stable and reliable predictions in the presence of noisy or erroneous data.\n",
        "\n",
        "4. Handling Complex Relationships: In complex datasets with nonlinear relationships or high-dimensional feature spaces, individual models may struggle to capture all the underlying patterns. Ensemble methods, by combining multiple models with different perspectives, can better capture complex relationships and improve overall predictive performance.\n",
        "\n",
        "5. Model Interpretability: While some ensemble methods like Random Forests provide insights into feature importance, the interpretability of ensemble techniques can vary depending on the specific method used. However, in certain cases, ensembles can provide interpretability by aggregating predictions from multiple interpretable models.\n",
        "\n",
        "6. Reduction of Bias and Variance: Ensemble techniques can help balance bias and variance in model predictions. By combining models with different biases, ensemble methods can reduce overall bias, while combining models with different variances can reduce overall variance, resulting in more accurate and reliable predictions.\n",
        "\n",
        "7. Increased Robustness: Ensemble methods are less sensitive to variations in the training data and the choice of hyperparameters. Even if some individual models perform poorly due to certain data characteristics or model settings, the ensemble as a whole can still provide reliable predictions.\n",
        "\n",
        "Overall, ensemble techniques are valuable tools in the machine learning toolkit, offering a powerful approach to improving predictive performance, robustness, and generalization across various types of datasets and modeling tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Noulk6U3xax"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcrNH-O14O29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Are ensemble techniques always better than individual models ?"
      ],
      "metadata": {
        "id": "VDnaDwmU4PSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques are powerful tools in machine learning, but whether they are always better than individual models depends on various factors, including the dataset characteristics, the choice of models, and the specific problem being solved. Here are some considerations:\n",
        "\n",
        "1. Complexity: Ensemble techniques typically involve training multiple models and combining their predictions, which can introduce additional computational complexity and resource requirements compared to training and deploying a single model. In scenarios where computational resources are limited or inference speed is critical, using ensemble techniques may not be feasible.\n",
        "\n",
        "2. Interpretability: Ensemble models are generally less interpretable than individual models, especially when using complex ensemble methods like stacking or gradient boosting. If interpretability is a crucial requirement, using a single, interpretable model may be preferred over an ensemble approach.\n",
        "\n",
        "3. Data Quality: Ensemble techniques can help improve model robustness to noisy or low-quality data by leveraging multiple models' predictions. However, if the dataset is clean and well-structured, a single model may achieve high accuracy without the need for ensemble methods.\n",
        "\n",
        "4. Model Selection and Tuning: Building and tuning ensemble models often require careful selection of base learners, hyperparameter tuning, and optimization of ensemble strategies. In some cases, individual models with proper tuning may outperform ensembles, especially if the dataset is not highly complex or diverse.\n",
        "\n",
        "5. Overfitting: While ensemble techniques can help reduce overfitting compared to individual models, they are not immune to overfitting, especially if the base learners are too complex or if the ensemble is trained on a small dataset. Regularization techniques and careful model selection are essential to mitigate overfitting in ensemble models.\n",
        "\n",
        "6. Resource Constraints: In certain scenarios, such as edge devices or embedded systems, deploying ensemble models may not be practical due to resource constraints. In such cases, using lightweight individual models may be a more suitable option.\n",
        "\n",
        "In summary, while ensemble techniques can often lead to improved performance and robustness compared to individual models, they are not universally superior in all situations. It's essential to carefully consider the specific characteristics of the problem, the dataset, and the available resources when deciding whether to use ensemble techniques or individual models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6pxxFK1d4VPk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAGhKTyf4tG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the confidence interval calculate using bootstrap ?"
      ],
      "metadata": {
        "id": "3DkVHewN4tyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In bootstrap resampling, confidence intervals can be calculated by repeatedly sampling from the observed data and estimating the statistic of interest from each sample. The confidence interval is then derived from the distribution of these estimated statistics. Here's a step-by-step process for calculating a confidence interval using the bootstrap method:\n",
        "\n",
        "1. Sample with Replacement: From the original dataset of size n, randomly sample n data points with replacement. This new sample is called a bootstrap sample.\n",
        "\n",
        "2. Calculate Statistic: Compute the statistic of interest (e.g., mean, median, standard deviation, etc.) on the bootstrap sample. This statistic can be anything you want to estimate about the population, such as the mean, median, variance, or any other parameter.\n",
        "\n",
        "3. Repeat: Repeat steps 1 and 2 a large number of times (typically thousands of times) to create a distribution of the statistic.\n",
        "\n",
        "4. Calculate Confidence Interval: Determine the range of the distribution of the statistic that encompasses the desired level of confidence. The lower and upper bounds of this range form the confidence interval.\n",
        "\n",
        "The width of the confidence interval and its position around the point estimate (e.g., mean) provide information about the uncertainty associated with the estimated statistic"
      ],
      "metadata": {
        "id": "F7qJ6XWr439v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CjvhOZsU5d_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How  does bootstrap work and what are the steps involved in bootstrap ?"
      ],
      "metadata": {
        "id": "tMAjY7D75e31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic by repeatedly sampling from the observed data with replacement. The basic idea is to generate multiple bootstrap samples from the original dataset, compute the statistic of interest on each sample, and then analyze the distribution of these statistics to make inferences about the population parameter.\n",
        "\n",
        "Here are the steps involved in the bootstrap method:\n",
        "\n",
        "1. Original Sample: Start with a dataset containing n observations.\n",
        "\n",
        "2. Sampling with Replacement: Randomly select n observations from the original dataset, allowing for replacement. Some observations may be selected multiple times, while others may not be selected at all.\n",
        "\n",
        "3. Compute Statistic: Calculate the statistic of interest (e.g., mean, median, standard deviation, etc.) on the bootstrap sample. This statistic can be anything you want to estimate about the population, such as a parameter or the performance of a model.\n",
        "\n",
        "4. Repeat: Repeat steps 2 and 3 a large number of times (typically thousands of times) to create multiple bootstrap samples and compute the statistic for each sample.\n",
        "\n",
        "5. Analyze Distribution: Analyze the distribution of the computed statistics. This can be done by constructing histograms, computing summary statistics (e.g., mean, median, standard deviation), or plotting the empirical cumulative distribution function (ECDF).\n",
        "\n",
        "6. Inference: Use the distribution of the computed statistics to make inferences about the population parameter. For example, you can estimate the mean of the population, construct confidence intervals, perform hypothesis tests, or assess the variability of a model's performance.\n",
        "\n",
        "The key idea behind bootstrap is that by repeatedly sampling from the observed data, we can simulate the process of drawing multiple samples from the population. This allows us to estimate the sampling distribution of a statistic without making strong assumptions about the underlying population distribution. Bootstrap is particularly useful when analytical methods for deriving the sampling distribution are unavailable or difficult to compute. It provides a straightforward and computationally efficient approach for estimating uncertainty and making statistical inferences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7rFGpWKi5lnF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_0-iOTds6Co6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
      ],
      "metadata": {
        "id": "rrAcN7VnqTat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original sample data\n",
        "original_sample_mean = 15\n",
        "original_sample_std = 2\n",
        "n = 50\n",
        "B = 1000  # Number of bootstrap samples\n",
        "\n",
        "# Generate the original sample\n",
        "np.random.seed(42)  # For reproducibility\n",
        "original_sample = np.random.normal(loc=original_sample_mean, scale=original_sample_std, size=n)\n",
        "\n",
        "# Function to generate bootstrap samples and calculate their means\n",
        "bootstrap_means = []\n",
        "for _ in range(B):\n",
        "    bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n",
        "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
        "\n",
        "# Calculate the 95% confidence interval\n",
        "bootstrap_means = np.array(bootstrap_means)\n",
        "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
        "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
        "\n",
        "(lower_bound, upper_bound)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wTPUwllqdRd",
        "outputId": "89d6735d-9d0e-4142-ff44-e1a5ee97a074"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14.030784976768311, 15.09011954764033)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9aLC7WXqj6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}