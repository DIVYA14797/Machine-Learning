{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvBFYg8Nk+iuYy4NmPrg6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIVYA14797/Machine-Learning/blob/main/Baye's.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhoDuneUTWLv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Baye's theorem  ?\n",
        "\n"
      ],
      "metadata": {
        "id": "MTK530rvTx_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' Theorem, named after the Reverend Thomas Bayes, is a fundamental theorem in probability theory that describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Mathematically, Bayes' Theorem is stated as follows:\n",
        "$$P(A∣B)=\\frac{P(B∣A)×P(A)}{P(B)}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* P(A∣B) is the probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
        "* P(B∣A) is the probability of event B occurring given that event A has occurred. This is called the likelihood.\n",
        "* P(A) is the probability of event A occurring independent of event B. This is called the prior probability.\n",
        "* P(B) is the probability of event B occurring independent of event A. This is called the evidence or marginal likelihood.\n",
        "\n",
        "In words, Bayes' Theorem tells us how to update our beliefs about the probability of event A occurring given new evidence B. It provides a formal framework for incorporating new information into our existing knowledge to arrive at a more informed estimate of the probability of an event.\n",
        "\n",
        "Bayes' Theorem has wide-ranging applications across various fields, including statistics, machine learning, Bayesian inference, medical diagnosis, spam filtering, and more. It forms the basis of Bayesian statistics, where it is used to update prior beliefs based on observed data to make probabilistic inferences."
      ],
      "metadata": {
        "id": "YADZ9JMnT2X-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJoBxch0VJxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the formula for Baye's theorem ?\n",
        "\n"
      ],
      "metadata": {
        "id": "rQiTIs-OVK9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematically, Bayes' Theorem is stated as follows:\n",
        "$$P(A∣B)=\\frac{P(B∣A)×P(A)}{P(B)}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* P(A∣B) is the probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
        "* P(B∣A) is the probability of event B occurring given that event A has occurred. This is called the likelihood.\n",
        "* P(A) is the probability of event A occurring independent of event B. This is called the prior probability.\n",
        "* P(B) is the probability of event B occurring independent of event A. This is called the evidence or marginal likelihood.\n"
      ],
      "metadata": {
        "id": "wGy3ggnSVjuM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgHg8cvlV39i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How  is Baye's theorem used in practice ?"
      ],
      "metadata": {
        "id": "Fz83MivOV4jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' Theorem is used in various practical applications across different fields. Here are some common ways it is applied in practice:\n",
        "\n",
        "1. Bayesian Inference:\n",
        "\n",
        "* Bayes' Theorem is fundamental to Bayesian statistics, where it is used to update prior beliefs or probabilities based on observed data.\n",
        "* In Bayesian inference, prior beliefs about the likelihood of different outcomes are combined with observed data to calculate posterior probabilities, which represent updated beliefs after considering the evidence.\n",
        "\n",
        "2. Medical Diagnosis:\n",
        "\n",
        "* Bayes' Theorem is used in medical diagnosis to estimate the probability of a disease given certain symptoms or test results.\n",
        "* Doctors can use prior knowledge about the prevalence of a disease and the accuracy of diagnostic tests to calculate the probability of a patient having the disease based on their symptoms or test results.\n",
        "\n",
        "3. Spam Filtering:\n",
        "\n",
        "* Bayes' Theorem is used in email spam filtering to classify incoming emails as either spam or non-spam (ham).\n",
        "* By analyzing the frequency of certain words or phrases in spam and non-spam emails (likelihood), along with the overall prevalence of spam emails (prior probability), Bayes' Theorem can be used to calculate the probability that a given email is spam.\n",
        "\n",
        "4. Machine Learning:\n",
        "\n",
        "* In machine learning, Bayes' Theorem is used in Bayesian classifiers, such as Naive Bayes, which are based on the assumption of strong (naive) independence between features.\n",
        "* These classifiers calculate the probability of a given class label given the observed feature values using Bayes' Theorem and make predictions based on the class with the highest probability.\n",
        "\n",
        "5. Search Engines:\n",
        "\n",
        "* Bayes' Theorem can be used in search engines to improve the relevance of search results.\n",
        "* By analyzing the frequency of certain keywords in documents (likelihood) and the overall prevalence of relevant documents (prior probability), Bayes' Theorem can be used to rank search results based on their relevance to a given query.\n",
        "\n",
        "6. Risk Assessment:\n",
        "\n",
        "* Bayes' Theorem is used in risk assessment and decision-making processes to estimate the likelihood of different outcomes based on available evidence.\n",
        "* It helps in quantifying uncertainties and making informed decisions by incorporating prior knowledge and updating beliefs based on new information.\n",
        "\n",
        "These are just a few examples, and Bayes' Theorem has many other applications in fields such as finance, engineering, natural language processing, and more. Its versatility and ability to incorporate prior knowledge with new evidence make it a powerful tool for probabilistic reasoning and decision-making."
      ],
      "metadata": {
        "id": "GzzS4jHxWiuv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zd4NbsmIXS3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the relationship between Baye's theorem and conditional probability ?"
      ],
      "metadata": {
        "id": "rXmlbuJrXT2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' Theorem and conditional probability are closely related concepts in probability theory, and Bayes' Theorem can be derived from the definition of conditional probability.\n",
        "\n",
        "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted as P(A∣B), which reads as \"the probability of event A occurring given that event B has occurred\".\n",
        "\n",
        "Bayes' Theorem provides a way to calculate conditional probabilities by reversing the order of conditioning. It states:\n",
        "$$P(A∣B)=\\frac{P(B∣A)×P(A)}{P(B)}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* P(A∣B) is the probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
        "* P(B∣A) is the probability of event B occurring given that event A has occurred. This is called the likelihood.\n",
        "* P(A) is the probability of event A occurring independent of event B. This is called the prior probability.\n",
        "* P(B) is the probability of event B occurring independent of event A. This is called the evidence or marginal likelihood.\n",
        "\n",
        "In essence, Bayes' Theorem allows us to calculate the probability of event A given evidence B, by combining our prior knowledge about event A (prior probability) with new evidence B (likelihood) and the overall probability of observing evidence B (marginal likelihood).\n",
        "\n",
        "So, the relationship between Bayes' Theorem and conditional probability is that Bayes' Theorem provides a formula for calculating conditional probabilities by considering prior probabilities and likelihoods. It's a powerful tool for updating beliefs or probabilities based on observed data or evidence."
      ],
      "metadata": {
        "id": "UfeYHzdGXhZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmfSMVJ7YDaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do you choose which type of Naive Baye's classifier to use for any given problem ?"
      ],
      "metadata": {
        "id": "1gWFgkYfYEGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the appropriate type of Naive Bayes classifier depends on the characteristics of the problem and the nature of the data. There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's how you can decide which one to use:\n",
        "\n",
        "1. Gaussian Naive Bayes:\n",
        "\n",
        "* Use Gaussian Naive Bayes when the features are continuous or real-valued.\n",
        "* It assumes that the likelihood of the features follows a normal (Gaussian) distribution.\n",
        "* This classifier is suitable for data where the features can be represented by continuous values, such as measurements or sensor data.\n",
        "\n",
        "2. Multinomial Naive Bayes:\n",
        "\n",
        "* Use Multinomial Naive Bayes when the features are categorical or represent counts.\n",
        "* It is commonly used for text classification tasks where features are word counts or term frequencies.\n",
        "* Multinomial Naive Bayes assumes that the features are multinomially distributed.\n",
        "\n",
        "3. Bernoulli Naive Bayes:\n",
        "\n",
        "* Use Bernoulli Naive Bayes when the features are binary or boolean.\n",
        "* It is suitable for binary feature vectors, where each feature represents the presence or absence of a particular attribute.\n",
        "* Bernoulli Naive Bayes assumes that the features are Bernoulli distributed.\n",
        "\n",
        "When choosing the appropriate type of Naive Bayes classifier:\n",
        "\n",
        "* Consider the nature of the features: If the features are continuous, use Gaussian Naive Bayes; if they are categorical, use Multinomial Naive Bayes; if they are binary, use Bernoulli Naive Bayes.\n",
        "* Take into account any assumptions made by each classifier: Gaussian Naive Bayes assumes a normal distribution of features, Multinomial Naive Bayes assumes a multinomial distribution, and Bernoulli Naive Bayes assumes a Bernoulli distribution. Make sure these assumptions align with the characteristics of your data.\n",
        "* It's often a good idea to try different types of Naive Bayes classifiers and compare their performance using techniques such as cross-validation or holdout validation to determine which one works best for your specific problem.\n",
        "\n",
        "Overall, the choice of Naive Bayes classifier depends on the specific requirements and characteristics of your dataset, and experimentation may be necessary to find the most suitable one."
      ],
      "metadata": {
        "id": "M0nKKes1YlEB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9BPoVX0ZJDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Q6. Assignment:\n",
        "\n",
        "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
        "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
        "each feature value for each class:\n",
        "\n",
        "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
        "A 3 3 4 4 3 3 3\n",
        "B 2 2 1 2 2 2 3\n",
        "\n",
        "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
        "to belong to?"
      ],
      "metadata": {
        "id": "CHrBlByfbY4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the Naive Bayes classifier for predicting the class of a new instance with features $X_1=3 and X_2=4$, we need to calculate the posterior probabilities for each class given the feature values. We'll assume equal prior probabilities for each class.\n",
        "\n",
        "Step 1: Calculate the prior probabilities\n",
        "\n",
        "Since the prior probabilities are assumed to be equal:\n",
        "\n",
        "$P(A)=P(B)=0.5$\n",
        "\n",
        "Step 2: Calculate the likelihoods\n",
        "\n",
        "To calculate the likelihoods, we first need the total number of instances in each class.\n",
        "\n",
        "* Total instances in Class A:3+3+4=10 $(for X_1) and 4+3+3+3=13(for X_2)$\n",
        "* Total instances in Class A:2+2+1=5 $(for X_1) and 2+2+2+3=9(for X_2)$\n",
        "\n",
        "The likelihood of $X_1 = 3$ given Class A is:\n",
        "\n",
        "$P(X_1=3|A)=4/10=0.4$\n",
        "\n",
        "The likelihood of $X_1 = 3$ given Class B is:\n",
        "\n",
        "$P(X_1=3|B)=1/5=0.2$\n",
        "\n",
        "The likelihood of $X_2 = 4$ given Class A is:\n",
        "\n",
        "$P(X_2=4|A)=3/13≈0.2308$\n",
        "\n",
        "The likelihood of $X_2 = 4$ given Class B is:\n",
        "\n",
        "$P(X_2=4|B)=3/9=0.3333$\n",
        "\n",
        "Step 3: Calculate the posterior probabilities\n",
        "\n",
        "We use Bayes' theorem, but since the priors are equal, we can ignore the normalizing constant and just compare the numerators.\n",
        "\n",
        "For Class A:\n",
        "\n",
        "$P(A|X_1=3, X_2=4)∝P(X_1=3|A)⋅P(X_2=4|A)⋅P(A)$\n",
        "\n",
        "$P(A|X_1=3 , X_2=4)∝0.4⋅0.2308⋅0.5$\n",
        "\n",
        "$P(A|X_1=3 , X_2=4)∝0.04616$\n",
        "\n",
        "For Class B:\n",
        "\n",
        "$P(A|X_1=3, X_2=4)∝P(X_1=3|B)⋅P(X_2=4|B)⋅P(B)$\n",
        "\n",
        "$P(B|X_1=3 , X_2=4)∝0.2⋅0.3333⋅0.5$\n",
        "\n",
        "$P(B|X_1=3 , X_2=4)∝0.03333$\n",
        "\n",
        "Step 4: Compare the probabilities\n",
        "\n",
        "$P(A|X_1=3, X_2=4)≈0.04616$\n",
        "\n",
        "$P(B|X_1=3,X_2=4)≈0.03333$\n",
        "\n",
        "Since $P(A|X_1=3, X_2=4)$ is greater than $P(B|X_1=3,X_2=4)$ ,the Naive Bayes classifier will predict that the new instance belongs to Class A.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tMqBCa02bhrY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PvXMe3z6k6_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}